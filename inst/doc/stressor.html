<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>stressor</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">stressor</h1>



<p>This package is designed to allow the user to apply multiple machine
learning methods by calling simple commands for data exploration.
<code>Python</code> has a library, called <code>PyCaret</code>, which
uses pipeline processes for fitting multiple models with a few lines of
code. The <code>stressor</code> package uses the <code>reticulate</code>
package to allow <code>python</code> to be run in <code>R</code>, giving
access to the same tools that exist in <code>python</code>. One of the
strengths of <code>R</code> is exploration. The <code>stressor</code>
package gives you the freedom to explore the machine learning models
side by side.</p>
<p>To get started, <code>stressor</code> requires that you have
<code>Python</code> 3.8.10</a> installed on your computer. To install
<code>Python</code>, please follow the instructions provided at:</p>
<p style="text-align:center;">
<a href="https://www.python.org/downloads/release/python-3810/">
https://www.python.org/downloads/release/python-3810/</a>
</p>
<p>Once Python is installed, you can install <code>stressor</code> from
CRAN. For your convenience, we have attached <code>stressor</code> with
the <code>library</code> statement to use the <code>python</code>
features of <code>stressor.</code></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">library</span>(stressor)</span></code></pre></div>
<div id="data-generation" class="section level2">
<h2>Data Generation</h2>
<p>It is convenient when testing new functions or algorithms to be able
to generate toy data sets. With these toy data sets, we can choose the
distribution of the parameters, of the error term, and the underlying
model of the toy data set.</p>
<p>In this section, we will show an example of generating linear data
with an epsilon and intercept that we chose. We will generate 500
observations from a linear model with five independent variables and a
y-intercept of zero. Observations are simulated from this model assuming
that the residuals follow a normal distribution with a mean of zero and
a standard deviation of one. With respect to the variables chosen, each
variable is sampled from a normal distribution with mean zero and
standard deviation of one. For this case, we chose to let the
coefficients on each term be one, as we wanted each independent variable
to be equally weighted. When we create the response variable, Y, it is
the sum of each independent variable plus an epsilon term that is
sampled from a standard normal distribution.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">43421</span>)</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>lm_data <span class="ot">&lt;-</span> <span class="fu">data_gen_lm</span>(<span class="dv">500</span>, <span class="at">weight_vec =</span> <span class="fu">rep</span>(<span class="dv">1</span>, <span class="dv">5</span>), <span class="at">y_int =</span> <span class="dv">0</span>, <span class="at">resp_sd =</span> <span class="dv">1</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="fu">head</span>(lm_data)</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="co">#&gt;           Y         V1         V2         V3          V4          V5</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="co">#&gt; 1 1.5101730  0.9493875 -0.2231050  0.7501904  0.31629917 -0.41787475</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="co">#&gt; 2 2.0124439  1.4844310  1.0737816 -1.8404303  0.85267167 -0.96389423</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a><span class="co">#&gt; 3 2.6647624 -0.3505283 -0.3922640  0.7192181  0.05188511  1.60003509</span></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a><span class="co">#&gt; 4 3.9270489  2.2945235 -0.8998011  0.1046142  1.45699275  1.01588132</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a><span class="co">#&gt; 5 2.6975509  0.8574341 -0.9723329 -0.9897257  2.80821651  0.00363803</span></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a><span class="co">#&gt; 6 0.8071714  0.7676524 -1.2666080  0.5582797 -0.80401673  0.12742990</span></span></code></pre></div>
<div id="validation-of-data-generation" class="section level3">
<h3>Validation of Data Generation</h3>
<p>Below is a visual of when we know the standard deviation of the
epsilon term. We can show that our models fit the data if we are close
to the theoretical error. In the graphic below, the black dots represent
the value given the current epsilon that we are on. The red line
represents the expected theoretical error.
<img src="data:image/png;base64,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" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="machine-learning-model-workflow" class="section level2">
<h2>Machine Learning Model Workflow</h2>
<p>In this section, we will demonstrate a typical workflow using the
functions of this package to explore the machine learning models (mlm)
that are accessible through the <code>PyCaret</code> module in
<code>python</code>. First, we need to create a virtual environment for
the <code>PyCaret</code> module to exist in. The first time you run this
code it will take some time (~ 5 min), as it needs to install the
necessary modules into the virtual environment. Note that this virtual
environment will be about 1 GB of space on the user’s disk.
<code>PyCaret</code> recommends that its library be used in a virtual
environment. A virtual environment is a separate partition of
<code>python</code> that can have a specific <code>python</code> version
installed, as well as other <code>python</code> libraries. This enables
the tools needed to be contained without disturbing the main version of
<code>python</code> installed.</p>
<p>Once installed, the following message will be shown after you execute
the code indicating that you are now using the virtual environment.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="fu">create_virtualenv</span>()</span></code></pre></div>
<p>See the <a href="#troubleshoot">troubleshoot</a> section if other errors appear. The
only time you will need to install a new environment is if you decide to
delete a <code>stressor</code> environment and need to initiate a new
one. You do not need to install a new environment for each
<code>R</code> session, it is one and done. These environments are
stored inside the <code>python</code> module on your computer.</p>
<p>To begin using, we need to create all the mlm. This may take a moment
(&lt; 3 min) the first time you run it, as the <code>PyCaret</code>
module needs to be imported. Then depending on your data size it may
take a moment (&lt; 5 min for data &lt;10,000) to fit the data. Note
that console output will be shown and a progress bar will be displayed
showing the progress of the fitting.</p>
<p>For reproducibility, we have set the seed again and have defined a
new data set, and set the seed for the <code>python</code> side by
passing the seed to the function. Here are the commands:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">43421</span>)</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>lm_data <span class="ot">&lt;-</span> <span class="fu">data_gen_lm</span>(<span class="dv">1000</span>)</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a><span class="co"># Split the data into a 80/20 split</span></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>indices <span class="ot">&lt;-</span> <span class="fu">split_data_prob</span>(lm_data, .<span class="dv">8</span>)</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>train <span class="ot">&lt;-</span> lm_data[indices, ]</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>test <span class="ot">&lt;-</span> lm_data[<span class="sc">!</span>indices, ]</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a><span class="co"># Tune the models</span></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>mlm_lm <span class="ot">&lt;-</span> <span class="fu">mlm_regressor</span>(Y <span class="sc">~</span> ., lm_data, <span class="at">sort_v =</span> <span class="st">&#39;RMSE&#39;</span>, <span class="at">seed =</span> <span class="dv">43421</span>)</span></code></pre></div>
<p>Now, we can look at the initial training predictive accuracy measures
such as RMSE. The <code>mlm_lm</code> is a list object where the first
element is a list of all the models that were fitted. For example, if we
were to pass these models back to <code>PyCaret</code>, they can be
refitted or used again for predictions. The second element is a data
frame for the initial values and the corresponding models. If you want
to specify the models that are fitted, you can change the
<code>fit_models</code> parameter – a character vector – specifying the
models to be used. Also we can change how the models are sorted based
upon the metrics listed which is given to the <code>sort_v</code>
variable.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>mlm_lm<span class="sc">$</span>pred_accuracy</span></code></pre></div>
<pre><code>#&gt;                                    Model    MAE    MSE   RMSE      R2  RMSLE
#&gt; lr                     Linear Regression 0.8345 1.0955 1.0429  0.8261 0.3664
#&gt; ridge                   Ridge Regression 0.8344 1.0955 1.0429  0.8261 0.3664
#&gt; lar               Least Angle Regression 0.8345 1.0955 1.0429  0.8261 0.3664
#&gt; br                        Bayesian Ridge 0.8344 1.0955 1.0429  0.8261 0.3664
#&gt; huber                    Huber Regressor 0.8356 1.0976 1.0440  0.8259 0.3671
#&gt; gbr          Gradient Boosting Regressor 1.0308 1.6365 1.2731  0.7425 0.4293
#&gt; et                 Extra Trees Regressor 0.9922 1.6474 1.2785  0.7406 0.4308
#&gt; knn                K Neighbors Regressor 1.0231 1.6798 1.2936  0.7336 0.4390
#&gt; lightgbm Light Gradient Boosting Machine 1.0432 1.7054 1.3013  0.7303 0.4331
#&gt; rf               Random Forest Regressor 1.0448 1.7751 1.3253  0.7221 0.4406
#&gt; ada                   AdaBoost Regressor 1.1535 2.1419 1.4542  0.6656 0.4869
#&gt; par         Passive Aggressive Regressor 1.2356 2.4503 1.5240  0.6015 0.4654
#&gt; en                           Elastic Net 1.4439 3.3031 1.8107  0.4877 0.6173
#&gt; dt               Decision Tree Regressor 1.5140 3.6288 1.8966  0.4181 0.5561
#&gt; omp          Orthogonal Matching Pursuit 1.8988 5.6044 2.3593  0.1243 0.6988
#&gt; lasso                   Lasso Regression 1.9174 5.7881 2.3973  0.1022 0.9362
#&gt; llar        Lasso Least Angle Regression 1.9174 5.7881 2.3973  0.1022 0.9362
#&gt; dummy                    Dummy Regressor 2.0239 6.5123 2.5450 -0.0132 1.0254
#&gt;            MAPE TT (Sec)
#&gt; lr       1.5432    0.009
#&gt; ridge    1.5407    0.009
#&gt; lar      1.5432    0.008
#&gt; br       1.5405    0.009
#&gt; huber    1.5460    0.010
#&gt; gbr      1.8618    0.115
#&gt; et       1.6469    0.133
#&gt; knn      1.7811    0.009
#&gt; lightgbm 2.0825    0.037
#&gt; rf       1.6010    0.251
#&gt; ada      1.5053    0.069
#&gt; par      2.2490    0.009
#&gt; en       1.0218    0.009
#&gt; dt       2.2846    0.011
#&gt; omp      2.1174    0.009
#&gt; lasso    1.0678    0.009
#&gt; llar     1.0678    0.008
#&gt; dummy    1.0414    0.007</code></pre>
<p>We pulled out a test validation set and we can currently check the
accuracy measures of those predicted values, such as RMSE.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>pred_lm <span class="ot">&lt;-</span> <span class="fu">predict</span>(mlm_lm, test)</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="fu">score</span>(test<span class="sc">$</span>Y, pred_lm)</span></code></pre></div>
<pre><code>#&gt;               rmse       mae       mse            r2     rmsle     mape
#&gt; lr       0.9811816 0.7855389 0.9627174  0.8489721348 0.1575507 1.787733
#&gt; ridge    0.9814217 0.7857800 0.9631885  0.8488982396 0.1576890 1.785325
#&gt; lar      0.9811817 0.7855389 0.9627175  0.8489721181 0.1575508 1.787733
#&gt; br       0.9814372 0.7857951 0.9632189  0.8488934604 0.1576978 1.785174
#&gt; huber    0.9810649 0.7866087 0.9624883  0.8490080733 0.1576332 1.784634
#&gt; gbr      1.1519822 0.8913621 1.3270630  0.7918148288 0.1819330 1.779195
#&gt; et       1.2112827 0.9544208 1.4672058  0.7698296888 0.2063202 1.538019
#&gt; knn      1.2961473 1.0304869 1.6799978  0.7364476099 0.2127031 1.447863
#&gt; lightgbm 1.1740861 0.9132695 1.3784782  0.7837489759 0.1870156 1.385519
#&gt; rf       1.2685104 0.9956673 1.6091187  0.7475668762 0.2078816 1.752251
#&gt; ada      1.4734451 1.1557082 2.1710406  0.6594144709 0.2363207 1.464789
#&gt; par      2.2243526 1.8216665 4.9477444  0.2238145292 0.2836395 3.536359
#&gt; en       1.7919837 1.4126925 3.2112056  0.4962368798 0.2888262 1.161516
#&gt; dt       1.9384331 1.4986607 3.7575229  0.4105324586 0.3019004 1.988530
#&gt; omp      2.2605309 1.7922139 5.1100000  0.1983604118 0.3411484 1.932450
#&gt; lasso    2.3843600 1.8606841 5.6851724  0.1081293000 0.3579984 1.037209
#&gt; llar     2.3843599 1.8606841 5.6851724  0.1081293122 0.3579984 1.037209
#&gt; dummy    2.5257071 1.9860032 6.3791965 -0.0007468551 0.3739615 1.065824</code></pre>
<p>In comparison, we can fit this data using the <code>lm()</code>
function and check the initial predictive accuracy with simple test
data.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>test_index <span class="ot">&lt;-</span> <span class="fu">split_data_prob</span>(lm_data, .<span class="dv">2</span>)</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>test <span class="ot">&lt;-</span> lm_data[test_index, ]</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>train <span class="ot">&lt;-</span> lm_data[<span class="sc">!</span>test_index, ]</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>lm_test <span class="ot">&lt;-</span> <span class="fu">lm</span>(Y <span class="sc">~</span> ., train)</span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>lm_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(lm_test, <span class="at">newdata =</span> test)</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>lm_score <span class="ot">&lt;-</span> <span class="fu">score</span>(test<span class="sc">$</span>Y, lm_pred)</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a>lm_score</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a><span class="co">#&gt;      RMSE       MAE       MSE        R2     RMSLE      MAPE </span></span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a><span class="co">#&gt; 0.9716537 0.7793268 0.9441110 0.8095243 0.1563934 1.0339178</span></span></code></pre></div>
<p>As we look at this initial result, we see that there are some
comparable models to the RMSE generated from <code>lm()</code> (which is
0.97 compared to 0.98 fitted by Huber Regressor). We see that the mlm
outperforms the models that were fitted by <code>lm()</code>. However,
it is not clear from this output alone whether the better performance
observed from the lm model is statistically significant. A better
practice would be performing a cross-validation.</p>
<p>In this code we are fitting the <code>mlm_lm</code> and
<code>lm_test</code> to the <code>lm_data</code> using a 10 fold
cross-validation.</p>
<p>First the ML models:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>mlm_cv <span class="ot">&lt;-</span> <span class="fu">cv</span>(mlm_lm, lm_data, <span class="at">n_folds =</span> <span class="dv">10</span>)</span></code></pre></div>
<p>Then the <code>lm_test</code>:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>lm_cv <span class="ot">&lt;-</span> <span class="fu">cv</span>(lm_test, lm_data, <span class="at">n_folds =</span> <span class="dv">10</span>)</span></code></pre></div>
<p>Now to compare the corresponding RMSE.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="fu">score</span>(lm_data<span class="sc">$</span>Y, mlm_cv)</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="co">#&gt;              RMSE      MAE      MSE         R2     RMSLE     MAPE</span></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="co">#&gt; lr       2.364924 1.870219 5.592865 -0.9244870 0.2981139 2.776382</span></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a><span class="co">#&gt; ridge    2.363766 1.869422 5.587389 -0.9226029 0.2979252 2.773871</span></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a><span class="co">#&gt; lar      2.364924 1.870219 5.592865 -0.9244870 0.2981139 2.776382</span></span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a><span class="co">#&gt; br       2.363785 1.869435 5.587481 -0.9226344 0.2979288 2.773913</span></span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a><span class="co">#&gt; huber    2.353164 1.861261 5.537381 -0.9053951 0.2965040 2.761076</span></span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a><span class="co">#&gt; gbr      2.311607 1.828339 5.343527 -0.8386907 0.2905366 2.638235</span></span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a><span class="co">#&gt; et       2.269689 1.806269 5.151489 -0.7726111 0.2854914 2.508345</span></span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a><span class="co">#&gt; knn      2.335107 1.855689 5.452724 -0.8762651 0.2938159 2.636042</span></span>
<span id="cb12-11"><a href="#cb12-11" tabindex="-1"></a><span class="co">#&gt; lightgbm 2.372964 1.869536 5.630957 -0.9375943 0.2985878 2.837477</span></span>
<span id="cb12-12"><a href="#cb12-12" tabindex="-1"></a><span class="co">#&gt; rf       2.281818 1.817093 5.206693 -0.7916066 0.2870201 2.552018</span></span>
<span id="cb12-13"><a href="#cb12-13" tabindex="-1"></a><span class="co">#&gt; ada      2.186354 1.754228 4.780145 -0.6448326 0.2771474 2.193652</span></span>
<span id="cb12-14"><a href="#cb12-14" tabindex="-1"></a><span class="co">#&gt; par      2.576865 2.055230 6.640235 -1.2848838 0.2992653 3.583453</span></span>
<span id="cb12-15"><a href="#cb12-15" tabindex="-1"></a><span class="co">#&gt; en       2.124765 1.711973 4.514628 -0.5534691 0.2731174 1.411179</span></span>
<span id="cb12-16"><a href="#cb12-16" tabindex="-1"></a><span class="co">#&gt; dt       2.716927 2.166145 7.381690 -1.5400162 0.3503943 3.437621</span></span>
<span id="cb12-17"><a href="#cb12-17" tabindex="-1"></a><span class="co">#&gt; omp      2.421582 1.945890 5.864059 -1.0178041 0.3030696 1.884166</span></span>
<span id="cb12-18"><a href="#cb12-18" tabindex="-1"></a><span class="co">#&gt; lasso    2.355189 1.899553 5.546917 -0.9086763 0.3000909 1.058293</span></span>
<span id="cb12-19"><a href="#cb12-19" tabindex="-1"></a><span class="co">#&gt; llar     2.355189 1.899553 5.546917 -0.9086763 0.3000909 1.058293</span></span>
<span id="cb12-20"><a href="#cb12-20" tabindex="-1"></a><span class="co">#&gt; dummy    2.414994 1.947364 5.832195 -1.0068397 0.3066178 1.023737</span></span>
<span id="cb12-21"><a href="#cb12-21" tabindex="-1"></a><span class="fu">score</span>(lm_data<span class="sc">$</span>Y, lm_cv)</span>
<span id="cb12-22"><a href="#cb12-22" tabindex="-1"></a><span class="co">#&gt;      RMSE       MAE       MSE        R2     RMSLE      MAPE </span></span>
<span id="cb12-23"><a href="#cb12-23" tabindex="-1"></a><span class="co">#&gt; 1.0640627 0.8360287 1.1322295 0.8052017 0.1406658 1.5685498</span></span></code></pre></div>
<p>We can see that the top five ML models are close in value to the
linear model.</p>
</div>
<div id="real-data-example" class="section level2">
<h2>Real Data Example</h2>
<p>We want to show how our functions apply to a real data example. We
can simulate data, but it is never quite like observed data. The purpose
of this data set is to show the use of the functions in this package –
specifically cross-validation. This is crucial to show how these work in
comparison to existing functions.</p>
<p>We will be using the Boston Housing Data from the
<code>mlbench</code> package. There are two versions of this data, the
second version includes a corrected <code>medv</code> value,
standardizing the median income to USD 1000’s. As some of the original
data was missing. This data version also has had the town, tract,
longitude and latitude added. For this analysis, we are ignoring spatial
autocorrelation and therefore will be removing these variables from the
analysis.</p>
<p>This next code chunk opens the cleaned Boston data set attached to
this package and fits the initial machine learning models. It then
displays the initial values from the first fit.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="fu">data</span>(boston)</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>mlm_boston <span class="ot">&lt;-</span> <span class="fu">mlm_regressor</span>(cmedv <span class="sc">~</span> ., boston)</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>mlm_boston<span class="sc">$</span>pred_accuracy</span></code></pre></div>
<pre><code>#&gt;                                    Model    MAE     MSE   RMSE      R2  RMSLE
#&gt; gbr          Gradient Boosting Regressor 2.1218  9.7524 3.0077  0.8615 0.1393
#&gt; et                 Extra Trees Regressor 2.1753 11.2063 3.1583  0.8453 0.1414
#&gt; rf               Random Forest Regressor 2.2152 11.2131 3.2292  0.8441 0.1467
#&gt; lightgbm Light Gradient Boosting Machine 2.4390 13.9694 3.6343  0.8122 0.1570
#&gt; ada                   AdaBoost Regressor 2.7002 15.1353 3.7275  0.7950 0.1755
#&gt; dt               Decision Tree Regressor 3.0190 20.6916 4.4073  0.7180 0.2007
#&gt; lr                     Linear Regression 3.3687 24.0099 4.7956  0.6858 0.2453
#&gt; ridge                   Ridge Regression 3.3493 24.0915 4.7963  0.6849 0.2513
#&gt; lar               Least Angle Regression 3.4298 24.4397 4.8504  0.6785 0.2473
#&gt; br                        Bayesian Ridge 3.3931 24.7832 4.8727  0.6777 0.2589
#&gt; huber                    Huber Regressor 3.3622 27.7117 5.0719  0.6496 0.2931
#&gt; en                           Elastic Net 3.5681 27.9055 5.1803  0.6461 0.2562
#&gt; lasso                   Lasso Regression 3.6315 29.0143 5.2788  0.6328 0.2506
#&gt; llar        Lasso Least Angle Regression 3.6315 29.0141 5.2788  0.6328 0.2506
#&gt; knn                K Neighbors Regressor 3.9844 33.5862 5.7336  0.5557 0.2237
#&gt; omp          Orthogonal Matching Pursuit 5.5777 62.2987 7.7159  0.2226 0.3140
#&gt; dummy                    Dummy Regressor 6.4549 78.6894 8.7760 -0.0148 0.3798
#&gt; par         Passive Aggressive Regressor 7.1163 83.3044 8.9282 -0.1049 0.4482
#&gt;            MAPE TT (Sec)
#&gt; gbr      0.1106    0.122
#&gt; et       0.1115    0.151
#&gt; rf       0.1148    0.263
#&gt; lightgbm 0.1197    0.056
#&gt; ada      0.1439    0.091
#&gt; dt       0.1550    0.035
#&gt; lr       0.1706    0.035
#&gt; ridge    0.1708    0.035
#&gt; lar      0.1737    0.035
#&gt; br       0.1730    0.035
#&gt; huber    0.1731    0.051
#&gt; en       0.1724    0.035
#&gt; lasso    0.1735    0.034
#&gt; llar     0.1735    0.036
#&gt; knn      0.1832    0.033
#&gt; omp      0.2728    0.032
#&gt; dummy    0.3508    0.032
#&gt; par      0.3716    0.034</code></pre>
<p>Observe the initial values for the Boston data set. Now compare these
to the cross-validated values.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>mlm_boston_cv <span class="ot">&lt;-</span> <span class="fu">cv</span>(mlm_boston, boston, <span class="at">n_folds =</span> <span class="dv">10</span>)</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>mlm_boston_score <span class="ot">&lt;-</span> <span class="fu">score</span>(boston<span class="sc">$</span>cmedv, mlm_boston_cv)</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a>mlm_boston_score</span></code></pre></div>
<pre><code>#&gt;              RMSE      MAE       MSE           R2      RMSLE      MAPE
#&gt; gbr      2.880214 2.096367  8.295632  0.901413505 0.03826106 0.1097795
#&gt; et       3.095790 2.038773  9.583918  0.886103331 0.04008861 0.1035976
#&gt; rf       3.091200 2.145828  9.555518  0.886440846 0.04044387 0.1106439
#&gt; lightgbm 3.223722 2.129938 10.392383  0.876495421 0.04176346 0.1082107
#&gt; ada      3.582974 2.773172 12.837703  0.847434883 0.04837222 0.1497296
#&gt; dt       4.485803 2.837352 20.122431  0.760862124 0.05795620 0.1437426
#&gt; lr       4.908502 3.451202 24.093388  0.713670689 0.06492690 0.1745373
#&gt; ridge    4.931873 3.439230 24.323376  0.710937485 0.06533195 0.1745591
#&gt; lar      4.909012 3.469367 24.098402  0.713611106 0.06523700 0.1759787
#&gt; br       5.006509 3.486602 25.065131  0.702122364 0.06636305 0.1770225
#&gt; huber    5.584459 3.587230 31.186187  0.629378842 0.07572677 0.1812409
#&gt; en       5.328285 3.710624 28.390621  0.662601752 0.06863061 0.1788665
#&gt; lasso    5.386974 3.754152 29.019492  0.655128160 0.06925798 0.1803929
#&gt; llar     5.386960 3.754145 29.019338  0.655129997 0.06925790 0.1803927
#&gt; knn      5.836711 4.009170 34.067194  0.595140547 0.07285509 0.1819197
#&gt; omp      8.116394 5.873471 65.875855  0.217121821 0.10323051 0.2868320
#&gt; dummy    9.183814 6.643323 84.342437 -0.002337712 0.11942902 0.3630920
#&gt; par      8.937533 6.676229 79.879500  0.050700482 0.12318012 0.3213036</code></pre>
<p>Clustered cross-validation is subsetting the parameter space into
groups that share similar attributes with one another. Therefore, if we
train on those groups the other group should fit similarly across the
test group.</p>
<p>Now, compare to the clustered cross-validation:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>mlm_boston_clust_cv <span class="ot">&lt;-</span> <span class="fu">cv</span>(mlm_boston, boston, <span class="at">n_folds =</span> <span class="dv">10</span>, <span class="at">k_mult =</span> <span class="dv">5</span>)</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>mlm_boston_clust_score <span class="ot">&lt;-</span> <span class="fu">score</span>(boston<span class="sc">$</span>cmedv, mlm_boston_clust_cv)</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a>mlm_boston_clust_score</span></code></pre></div>
<pre><code>#&gt;               RMSE       MAE       MSE         R2      RMSLE      MAPE
#&gt; gbr       3.752646  2.722356  14.08235  0.8326433 0.04915424 0.1408009
#&gt; et        3.665735  2.566496  13.43761  0.8403055 0.04730942 0.1309730
#&gt; rf        4.154256  2.798413  17.25785  0.7949053 0.05368193 0.1450138
#&gt; lightgbm  4.023057  2.752783  16.18499  0.8076553 0.05210606 0.1408062
#&gt; ada       4.433633  3.332406  19.65710  0.7663922 0.05852669 0.1778279
#&gt; dt        5.394398  3.608300  29.09953  0.6541770 0.06953791 0.1882557
#&gt; lr        5.879360  4.278917  34.56687  0.5892023 0.07986385 0.2287874
#&gt; ridge     5.741469  4.099545  32.96447  0.6082455 0.07815901 0.2207254
#&gt; lar       6.092678  4.370884  37.12072  0.5588520 0.08399236 0.2395658
#&gt; br        5.808605  4.075319  33.73989  0.5990303 0.07884975 0.2192994
#&gt; huber     6.444105  4.469043  41.52649  0.5064932 0.08726243 0.2302259
#&gt; en        5.900504  4.247760  34.81595  0.5862423 0.07639839 0.2110481
#&gt; lasso     6.105984  4.411099  37.28304  0.5569229 0.07907545 0.2195190
#&gt; llar      6.106000  4.411162  37.28323  0.5569207 0.07907643 0.2195278
#&gt; knn       8.027975  5.668933  64.44838  0.2340862 0.10082338 0.2561170
#&gt; omp       8.542734  6.256828  72.97830  0.1327154 0.10913893 0.3073734
#&gt; dummy     9.640141  7.050797  92.93233 -0.1044212 0.12579351 0.3862600
#&gt; par      21.356794 14.663207 456.11264 -4.4205085 0.18068971 0.8859428</code></pre>
<p>What we notice about this result is when we ignore spatial
autocorrelation and we compare the 10 fold cross-validation with the
clustered cross-validation, we see a general improvement in the values.
This suggests that maybe there is some other underlying factors,
i.e. spatial relationships.</p>
<p>The power to be able to explore is a compliment to the purpose of R.
With <code>stressor</code>, you are able to fit multiple machine
learning models with a few lines of code and perform 10 fold
cross-validation and clustered cross-validation. With a simple command,
you can return the values from the predictions.</p>
</div>
<div id="troubleshooting" class="section level2">
<h2>Troubleshooting</h2>
<p>When initiating the virtual environment, you may receive some errors
or warnings. <code>reticulate</code> has done a nice job with the error
handling of initiating the virtual environments. <code>reticulate</code>
is a package in <code>R</code> that handles the connection between
<code>R</code> and <code>python</code>.</p>
<p>For MacOS and Linux, please note that the
<code>create_virtualenv()</code> function will not work unless you have
<code>cmake</code>. <code>lightgbm</code> requires this compiler and
they have detailed instructions of how to install it, see
<a href="https://github.com/microsoft/LightGBM/blob/master/docs/Installation-Guide.rst">
here</a>.</p>
<p>If your system is not recognizing the <code>python</code> path that
you have, you will need to add it to your system variables, or specify
initially the python path that <code>create_virtualenv()</code> needs to
use. If you are still having trouble getting the virtual environment to
start you can use <code>reticulate</code>’s function
<code>reticulate::use_virtualenv()</code>. It also helps sometimes to
unset the <code>RETICULATE_PYTHON</code> variable. Also note that if the
environment has <code>python</code> objects in it the user will have to
clear them to restart the <code>reticulate</code> <code>python</code>
version.</p>
<p>If you receive a warning that says</p>
<p style="text-align:center; color:darkred">
“Warning Message: Previous request to use_python() … will be ignored. It
is superseded by request to use_python()”
</p>
<p>If the second <code>use_python</code> command has the matching
virtual environment you can ignore this warning and continue with your
analysis.</p>
<p>If you receive an error stating</p>
<p style="text-align:center; color:darkred">
ERROR: The requested version of Python … cannot be used, as another
version of Python … has already been initialized. Please restart the R
session if you need to attach reticulate to a different version of
Python.
</p>
<p>If this error appears, restart your R session and make sure to clear
all <code>python</code> objects. Then run the
<code>create_virtualenv()</code> function again. There should be no
problems attaching it after that, as long as your environment does not
contain any <code>Python</code> objects.</p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
